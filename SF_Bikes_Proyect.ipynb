{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF Bay Area Bike Share - Proyecto PAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre: Luis Alberto Webster Gil\n",
    "### Fecha: 06/10/2018\n",
    "### ITESO A.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente archivo tiene como finalidad explicar al usuario el uso de bicicletas en el área de San Francisco y las ciudades cercanas a esta. Recordemos que el objetivo de estas bicicletas es resolver problemas de transporte para distancias cortas dentro de la misma ciudad. \n",
    "\n",
    "La estructura de este reporte se compone de la siguiente manera:\n",
    "\n",
    "    1. Importación y obtención de datos\n",
    "    2. Exploración de datasets\n",
    "        2.1 Quality Report\n",
    "        2.2 Gráficas descriptivas de las variables\n",
    "        2.3 Comentarios basados en las gráficas generadas\n",
    "    3. Modelo de Machine Learning\n",
    "    4. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación y obtención de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos utilizados fueron obtenidos de Kaggle (https://www.kaggle.com/benhamner/sf-bay-area-bike-share#trip.csv y se componen de 4 diferentes datasets que contienen datos desde Agosto del 2013 hasta Agosto del 2015:\n",
    "\n",
    "    * station\n",
    "    * status\n",
    "    * trip\n",
    "    * weather\n",
    "\n",
    "Cada uno de estos datasets cuenta con un número diferente de registros y variables, lo cual añade un nivel de dificultad a la exploración de datos si se deseaba hacer de manera conjunta, es decir, en un solo dataframe. Con la finalidad de tener un mejor orden de los datasets y las variables a explorar, se decidió tener los 4 archivos csv en un diccionario llamado \"docs\". A continuación se presentan las líneas de código generadas para la importación de los datasets, paqueterías principales, work directory y funciones generadas por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sklearn.metrics as skm\n",
    "import scipy.spatial.distance as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "#%% Path donde se está trabajando\n",
    "path=\"C:\\\\Users\\\\LuisW\\\\Desktop\\\\ITESO\\\\XI Semestre\\\\Pap\"\n",
    "os.chdir(path) #os.getcwd() para verificar la ubicación del archivo a importar\n",
    "os.getcwd()\n",
    "\n",
    "#%% Funciones a utilizar en la base de datos\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "PARAM_TRIP_TIME = 86  # seconds\n",
    "\n",
    "\n",
    "def read_csv(csv_name):\n",
    "    if '.csv' not in csv_name:\n",
    "        csv_name += '.csv'\n",
    "    doc = pd.read_csv(csv_name)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def remove_unchanged_status(df, column_of_interest):\n",
    "    df_copy = deepcopy(df)\n",
    "\n",
    "    # Getting changes\n",
    "    df_copy['not_change'] = df_copy[column_of_interest].eq(\n",
    "        df_copy[column_of_interest].shift())\n",
    "\n",
    "    # Removing unchanged\n",
    "    df_copy = df_copy.loc[df_copy['not_change'] == 0]\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def transform_date(date, was_string=True):\n",
    "    if was_string:\n",
    "        return datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return datetime.datetime.strftime(date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "#%% Importar archivos csv en un diccionario\n",
    "if __name__ == '__main__':\n",
    "    csv_names = ['station', 'status', 'trip', 'weather']\n",
    "    docs = {csv_name: read_csv(csv_name=csv_name) for csv_name in csv_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe mencionar que las bases de datos tenían un peso aproximado de 660MB cuando estan comprimidas y de casi 5GB cuando se descomprimen. La base de datos con mayor número de registros fue status (aproximadamente 72 millones), razón por la cual se decidió crear un fragmento de código, el cual nos ayudaría a eliminar registros no necesarios del dataframe y así ganar velocidad en la ejecución del código en general. A continuación se presenta el código utilizado: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_unchanged_status(df, column_of_interest):\n",
    "    df_copy = deepcopy(df)\n",
    "\n",
    "    # Getting changes\n",
    "    df_copy['not_change'] = df_copy[column_of_interest].eq(\n",
    "        df_copy[column_of_interest].shift())\n",
    "\n",
    "    # Removing unchanged\n",
    "    df_copy = df_copy.loc[df_copy['not_change'] == 0]\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset Status se redujo considerablemente ya que ahora tiene una dimensión de poco más de 2 millones de registros, agilizando así la ejecución del código. A continuación se encuentra el fragmento de código que nos ayuda a eliminar los registros innecesarios, utilizando la función mencionada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100422"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Remover registros que no presentan cambio en Dataset Status.csv\n",
    "\n",
    "docs['status'] = remove_unchanged_status(df=docs.get('status'),\n",
    "                                             column_of_interest='bikes_available')\n",
    "\n",
    "new_length_status=len(docs['status'].bikes_available)\n",
    "new_length_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funciones para crear el quality report (meterlas en un .py)\n",
    "\n",
    "#importar funciones desde un archivo .py dentro de la misma carpeta)\n",
    "\n",
    "\n",
    "\n",
    "#lista de variables en la base de datos\n",
    "def columns_name(dataset):\n",
    "    return pd.DataFrame(list(dataset.columns.values))\n",
    "\n",
    "#lista de tipos de datos\n",
    "def data_types(dataset):\n",
    "    return pd.DataFrame(dataset.dtypes,columns=['Data Type'])\n",
    "\n",
    "#lista de datos nulos o faltantes\n",
    "def missing_values(dataset):\n",
    "    return pd.DataFrame(dataset.isnull().sum(),columns=['Missing_Values'])\n",
    "\n",
    "#número de datos en cada columna\n",
    "def present_data(dataset):\n",
    "    return pd.DataFrame(dataset.count(),columns=['Present_Data'])\n",
    "\n",
    "#valores únicos en la base de datos\n",
    "def unique_values(dataset):\n",
    "    unique_values_dataset = pd.DataFrame(columns=['Unique_Values'])\n",
    "    \n",
    "    for col in list(dataset.columns.values):\n",
    "        unique_values_dataset.loc[col]=[dataset[col].nunique()]\n",
    "        \n",
    "    return unique_values_dataset\n",
    "\n",
    "\n",
    "\n",
    "#valores mínimos de las columnas\n",
    "def minimum_values(dataset):\n",
    "    minimum_values_dataset = pd.DataFrame(columns=['Minimum_Values'])\n",
    "    \n",
    "    for col in list(dataset.columns.values):\n",
    "        try:\n",
    "            minimum_values_dataset.loc[col]=[dataset[col].min()]\n",
    "        except:\n",
    "            pass\n",
    "    return minimum_values_dataset\n",
    "\n",
    "\n",
    "\n",
    "#valores máximos de las columnas\n",
    "def maximum_values(dataset):\n",
    "    maximum_values_dataset = pd.DataFrame(columns=['Maximum_Values'])\n",
    "    \n",
    "    for col in list(dataset.columns.values):\n",
    "        try:\n",
    "            maximum_values_dataset.loc[col]=[dataset[col].max()]\n",
    "        except:\n",
    "            pass\n",
    "    return maximum_values_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Revisar función para crear el data quality report\n",
    "#función para crear el data quality report\n",
    "def data_quality_report(dataset):\n",
    "    results= data_types.join(missing_values).join(present_data).join(unique_values).join(minimum_values).join(maximum_values)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
